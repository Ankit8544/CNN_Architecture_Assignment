{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TOPIC: Understanding Pooling and Padding in CNN**\n",
    "\n",
    "1. **Describe the purpose and benefits of pooling in CNN.**\n",
    "\n",
    "2. **Explain the difference between min pooling and max pooling.**\n",
    "\n",
    "3. **Discuss the concept of padding in CNN and its significance.**\n",
    "\n",
    "4. **Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Purpose and Benefits of Pooling in CNN**\n",
    "\n",
    "    **Purpose -**\n",
    "    Pooling in a Convolutional Neural Network (CNN) is used to reduce the dimensionality of feature maps, helping in downsampling the input representation. It is applied after convolutional layers to reduce the number of parameters, computational load, and the chances of overfitting.\n",
    "\n",
    "    **Benefits -**\n",
    "\n",
    "    - **Dimensionality Reduction :** Pooling reduces the size of feature maps, which lowers memory usage and speeds up computation.\n",
    "    \n",
    "    - **Control Overfitting :** By reducing the number of parameters, pooling helps to control overfitting and improves generalization.\n",
    "    \n",
    "    - **Translation Invariance :** Pooling introduces some level of robustness to minor translations and distortions in the input image.\n",
    "    \n",
    "    - **Retains Important Features :** Pooling retains the most prominent features (such as edges or textures) while discarding less important information.\n",
    "\n",
    "2. **Difference Between Min Pooling and Max Pooling**\n",
    "\n",
    "    - **Max Pooling :** In max pooling, the maximum value within a defined window (e.g., 2x2) in the feature map is selected and retained. This emphasizes the strongest features in the region, such as high activations that may correspond to important edges or patterns.\n",
    "      \n",
    "    - **Min Pooling :** In min pooling, the minimum value within the pooling window is selected. This is much less common than max pooling and could be used in tasks where low activations are meaningful, but it's typically not favored in most CNN architectures.\n",
    "\n",
    "    **Key Difference -**\n",
    "\n",
    "    - **Max pooling** emphasizes the most significant activations (high intensity features).\n",
    "    \n",
    "    - **Min pooling** focuses on the least significant activations (low intensity features).\n",
    "\n",
    "3. **Concept of Padding in CNN and Its Significance**\n",
    "\n",
    "    **Padding** is the process of adding extra pixels (typically zeros) to the border of an image before performing the convolution operation. \n",
    "\n",
    "    **Significance -**\n",
    "\n",
    "    - **Preserves Spatial Dimensions :** Padding allows for control over the size of the output feature maps. Without padding, the feature map size reduces with each convolution, which might eliminate important edge information.\n",
    "    \n",
    "    - **Preserve Edge Features :** Padding helps in preserving edge information in images, which can otherwise get lost during convolution.\n",
    "    \n",
    "    - **Better Feature Extraction :** It allows the convolution operation to be applied over the entire image, even near the edges, leading to better feature extraction.\n",
    "\n",
    "4. **Comparison of Zero-Padding and Valid-Padding**\n",
    "\n",
    "    - **Zero-Padding -**\n",
    "\n",
    "      - **Definition :** Zero-padding adds extra rows and columns filled with zeros around the border of the input image. This preserves the spatial size of the input after convolution.\n",
    "    \n",
    "      - **Effect on Feature Map Size :** The output feature map size remains the same as the input when the appropriate amount of padding is applied (e.g., padding of size 1 for a 3x3 filter).\n",
    "    \n",
    "      - **Use Cases :** Zero-padding is typically used when the spatial size needs to be preserved or when edge features are considered important.\n",
    "\n",
    "    - **Valid-Padding -**\n",
    "\n",
    "      - **Definition :** Valid padding does not add any padding, meaning the convolution is only applied to valid parts of the image without extending beyond the image boundaries.\n",
    "    \n",
    "      - **Effect on Feature Map Size :** The feature map size is reduced compared to the input image because the filter cannot be applied to the border pixels.\n",
    "    \n",
    "      - **Use Cases :** Valid-padding is used when the focus is on reducing the spatial dimensions, leading to a more compact representation of the input.\n",
    "\n",
    "**Summary :**\n",
    "\n",
    "- **Zero-Padding** maintains the spatial dimensions of the feature map and preserves edge information.\n",
    "\n",
    "- **Valid-Padding** reduces the feature map size by excluding the border regions from the convolution operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TOPIC: Exploring LeNet**\n",
    "\n",
    "1. **Provide a brief overview of LeNet-5 architecture.**\n",
    "\n",
    "2. **Describe the key components of LeNet-5 and their respective purposes.**\n",
    "\n",
    "3. **Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks.**\n",
    "\n",
    "4. **Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTorch) and train it on a publicly available dataset (e.g., MNIST). Evaluate its performance and provide insights.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Overview of LeNet-5 Architecture**\n",
    "\n",
    "    LeNet-5, developed by Yann LeCun in 1998, is one of the earliest and most well-known Convolutional Neural Networks (CNNs). It was primarily designed for handwritten digit recognition and was successfully applied to the MNIST dataset. LeNet-5 is a relatively simple yet powerful architecture that laid the foundation for modern CNN architectures. It consists of seven layers, including convolutional, pooling (subsampling), and fully connected layers, along with an output layer.\n",
    "\n",
    "2. **Key Components of LeNet-5 and Their Purposes**\n",
    "\n",
    "   1. **Input Layer (32x32x1) -**\n",
    "\n",
    "      - **Purpose :** This layer takes in a 32x32 grayscale image. For MNIST, the original images are resized from 28x28 to 32x32 for compatibility with LeNet-5.\n",
    "      \n",
    "   2. **C1 - First Convolutional Layer (6x28x28) -**\n",
    "\n",
    "      - **Purpose :** Applies six 5x5 filters (or kernels) to the input image, generating six feature maps of size 28x28. The purpose of this layer is to detect low-level features such as edges and corners.\n",
    "\n",
    "   3. **S2 - First Subsampling (Pooling) Layer (6x14x14) -**\n",
    "\n",
    "      - **Purpose :** This layer applies average pooling with a 2x2 filter and a stride of 2, reducing the spatial dimensions of each feature map by half (from 28x28 to 14x14). It helps in reducing dimensionality and preserving important information.\n",
    "      \n",
    "   4. **C3 - Second Convolutional Layer (16x10x10) -**\n",
    "\n",
    "      - **Purpose :** Applies sixteen 5x5 filters to the pooled feature maps, generating 16 feature maps of size 10x10. This layer extracts more complex patterns and features from the input.\n",
    "      \n",
    "   5. **S4 - Second Subsampling (Pooling) Layer (16x5x5) -**\n",
    "\n",
    "      - **Purpose :** Similar to S2, this layer performs average pooling, reducing the spatial dimensions from 10x10 to 5x5 for each of the 16 feature maps.\n",
    "\n",
    "   6. **C5 - Fully Connected Convolutional Layer (120x1x1) -**\n",
    "\n",
    "      - **Purpose :** This layer consists of 120 5x5 filters applied to the entire 5x5x16 output from S4, producing a 120-dimensional output vector. It acts as a fully connected layer.\n",
    "\n",
    "   7. **F6 - Fully Connected Layer (84 neurons) -**\n",
    "\n",
    "      - **Purpose :** This fully connected layer has 84 neurons. Each neuron is connected to all 120 outputs from C5. This layer performs high-level feature extraction.\n",
    "      \n",
    "   8. **Output Layer (10 neurons) -**\n",
    "\n",
    "      - **Purpose :** This is the final fully connected layer with 10 neurons (one for each class in the MNIST dataset). It uses softmax activation to output class probabilities.\n",
    "\n",
    "3. **Advantages and Limitations of LeNet-5 in Image Classification**\n",
    "\n",
    "   **Advantages:**\n",
    "   - **Pioneering Architecture:** LeNet-5 was one of the first CNNs that demonstrated the power of deep learning for image recognition tasks.\n",
    "   - **Efficient Design:** Despite its simplicity, LeNet-5 achieved high accuracy on tasks like digit recognition. The architecture was well-designed with alternating convolutional and subsampling layers, making it efficient in terms of computation.\n",
    "   - **Lightweight:** Due to its small number of parameters, LeNet-5 is lightweight and can run efficiently on devices with limited computational resources.\n",
    "\n",
    "   #### **Limitations:**\n",
    "   - **Limited Complexity:** LeNet-5's architecture is simple and not suitable for more complex datasets like ImageNet, which involve high-resolution images and a large number of object categories.\n",
    "   - **Shallow Depth:** LeNet-5 has a relatively shallow depth compared to modern CNN architectures like ResNet or VGG, which limits its ability to capture hierarchical features in larger datasets.\n",
    "   - **Lack of Modern Techniques:** The architecture does not incorporate modern techniques like batch normalization, ReLU activation (it uses sigmoid/tanh), or dropout, which help in regularization and improving training speed in more complex architectures.\n",
    "\n",
    "4. **LeNet-5 Implementation and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,840</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,164</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">850</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │           \u001b[38;5;34m156\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │         \u001b[38;5;34m2,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)            │        \u001b[38;5;34m30,840\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)             │        \u001b[38;5;34m10,164\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m850\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,426</span> (173.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m44,426\u001b[0m (173.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,426</span> (173.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m44,426\u001b[0m (173.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8304 - loss: 0.6181 - val_accuracy: 0.9558 - val_loss: 0.1504\n",
      "Epoch 2/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9570 - loss: 0.1404 - val_accuracy: 0.9705 - val_loss: 0.0923\n",
      "Epoch 3/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9726 - loss: 0.0850 - val_accuracy: 0.9783 - val_loss: 0.0688\n",
      "Epoch 4/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9807 - loss: 0.0609 - val_accuracy: 0.9813 - val_loss: 0.0597\n",
      "Epoch 5/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9849 - loss: 0.0506 - val_accuracy: 0.9822 - val_loss: 0.0524\n",
      "Epoch 6/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9883 - loss: 0.0380 - val_accuracy: 0.9825 - val_loss: 0.0544\n",
      "Epoch 7/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9884 - loss: 0.0360 - val_accuracy: 0.9849 - val_loss: 0.0468\n",
      "Epoch 8/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9907 - loss: 0.0293 - val_accuracy: 0.9845 - val_loss: 0.0461\n",
      "Epoch 9/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9931 - loss: 0.0236 - val_accuracy: 0.9858 - val_loss: 0.0449\n",
      "Epoch 10/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9940 - loss: 0.0200 - val_accuracy: 0.9854 - val_loss: 0.0423\n",
      "Test loss: 0.04232536628842354\n",
      "Test accuracy: 0.9854000210762024\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, AveragePooling2D, Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape the dataset to add the channel dimension (1 channel for grayscale images)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32')\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32')\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Build the LeNet-5 model\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional layer: 6 filters of size 5x5, tanh activation, followed by average pooling\n",
    "model.add(Conv2D(6, kernel_size=(5,5), activation='tanh', input_shape=(28, 28, 1), padding='valid'))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), strides=2, padding='valid'))\n",
    "\n",
    "# Second convolutional layer: 16 filters of size 5x5, tanh activation, followed by average pooling\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), activation='tanh', padding='valid'))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), strides=2, padding='valid'))\n",
    "\n",
    "# Flatten the output from the convolutional layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer: 120 units, tanh activation\n",
    "model.add(Dense(120, activation='tanh'))\n",
    "\n",
    "# Fully connected layer: 84 units, tanh activation\n",
    "model.add(Dense(84, activation='tanh'))\n",
    "\n",
    "# Output layer: 10 units for 10 classes, softmax activation\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model performance on the test data\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TOPIC: Analyzing AlexNet**\n",
    "\n",
    "1. **Present an overview of the AlexNet architecture.**\n",
    "\n",
    "2. **Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough performance.**\n",
    "\n",
    "3. **Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet.**\n",
    "\n",
    "4. **Implement AlexNet using a deep learning framework of your choice and evaluate its performance on a dataset of your choice.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Overview of AlexNet Architecture**\n",
    "\n",
    "    AlexNet, developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, was the architecture that revolutionized deep learning by winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. It significantly outperformed all previous methods, marking a turning point in computer vision. AlexNet consists of 8 layers—5 convolutional layers followed by 3 fully connected layers—and uses ReLU activations, dropout, and max pooling to achieve better generalization and performance.\n",
    "\n",
    "2. **Architectural Innovations Introduced in AlexNet**\n",
    "\n",
    "    - **ReLU Activation Function :** Instead of traditional activation functions like sigmoid or tanh, AlexNet used ReLU (Rectified Linear Unit), which accelerates training by alleviating the vanishing gradient problem.\n",
    "    \n",
    "    - **Dropout Regularization :** Dropout was introduced to reduce overfitting by randomly setting a fraction of the layer's output to zero during training, making the network more robust.\n",
    "\n",
    "    - **GPU Utilization :** AlexNet exploited the computational power of GPUs to accelerate training. It was one of the first large-scale networks that efficiently used GPUs, training on two GPUs simultaneously to split the computational load.\n",
    "\n",
    "    - **Overlapping Max Pooling :** AlexNet employed overlapping pooling regions (stride less than the filter size), which improved the model’s ability to generalize.\n",
    "\n",
    "    - **Data Augmentation :** To prevent overfitting, AlexNet used data augmentation techniques such as random cropping, flipping, and changes in contrast, brightness, and RGB intensity to artificially increase the training data.\n",
    "\n",
    "    - **Local Response Normalization (LRN) :** LRN was used to normalize the responses across adjacent feature maps, aiding in better generalization.\n",
    "\n",
    "3. **Role of Convolutional Layers, Pooling Layers, and Fully Connected Layers in AlexNet**\n",
    "\n",
    "    - **Convolutional Layers :** AlexNet has 5 convolutional layers that detect features like edges, textures, shapes, and objects from images. The first layer uses large filters (11x11), and later layers use smaller filters to extract more abstract representations.\n",
    "\n",
    "    - **Pooling Layers :** Max pooling layers follow the first, second, and fifth convolutional layers. They reduce the spatial dimensions of feature maps while retaining important features, aiding in translation invariance and reducing overfitting.\n",
    "\n",
    "    - **Fully Connected Layers :** The last three layers are fully connected layers, which serve as classifiers that combine the features learned in the convolutional layers and produce the final output. The final layer contains 1000 neurons (for the 1000 ImageNet classes) and uses softmax activation for classification.\n",
    "\n",
    "4. **AlexNet Implementation and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 4s/step - accuracy: 0.0169 - loss: 6.9348 - val_accuracy: 0.0098 - val_loss: 42.4068\n",
      "Epoch 2/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 4s/step - accuracy: 0.0288 - loss: 4.7003 - val_accuracy: 0.0098 - val_loss: 42.6280\n",
      "Epoch 3/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 4s/step - accuracy: 0.0729 - loss: 4.2564 - val_accuracy: 0.0255 - val_loss: 22.8650\n",
      "Epoch 4/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 4s/step - accuracy: 0.0880 - loss: 3.9477 - val_accuracy: 0.0353 - val_loss: 13.0494\n",
      "Epoch 5/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 3s/step - accuracy: 0.1284 - loss: 3.6232 - val_accuracy: 0.0255 - val_loss: 10.0421\n",
      "Epoch 6/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 4s/step - accuracy: 0.1601 - loss: 3.3954 - val_accuracy: 0.0255 - val_loss: 10.7104\n",
      "Epoch 7/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 4s/step - accuracy: 0.1970 - loss: 3.1443 - val_accuracy: 0.0255 - val_loss: 15.7959\n",
      "Epoch 8/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 4s/step - accuracy: 0.2584 - loss: 2.9305 - val_accuracy: 0.0216 - val_loss: 8.0639\n",
      "Epoch 9/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 3s/step - accuracy: 0.2546 - loss: 2.8517 - val_accuracy: 0.0216 - val_loss: 14.1527\n",
      "Epoch 10/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 4s/step - accuracy: 0.3366 - loss: 2.4445 - val_accuracy: 0.0549 - val_loss: 8.4930\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 844ms/step - accuracy: 0.0384 - loss: 8.5568\n",
      "Test Loss: 8.6281\n",
      "Test Accuracy: 0.0377\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load dataset\n",
    "data, info = tfds.load('oxford_flowers102', with_info=True, as_supervised=True)\n",
    "train_data = data['train']\n",
    "validation_data = data['validation']\n",
    "test_data = data['test']\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))  # Resize images to 224x224\n",
    "    image = image / 255.0  # Normalize images to [0, 1]\n",
    "    return image, tf.one_hot(label, depth=102)\n",
    "\n",
    "# Batch and prefetch the dataset\n",
    "train_data = train_data.map(preprocess).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "validation_data = validation_data.map(preprocess).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "test_data = test_data.map(preprocess).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Build the model (AlexNet)\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(96, kernel_size=(11, 11), strides=(4, 4), input_shape=(224, 224, 3)),\n",
    "    layers.ReLU(),\n",
    "    layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "    layers.Conv2D(256, kernel_size=(5, 5), strides=(1, 1), padding='same'),\n",
    "    layers.ReLU(),\n",
    "    layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "    layers.Conv2D(384, kernel_size=(3, 3), strides=(1, 1), padding='same'),\n",
    "    layers.ReLU(),\n",
    "    \n",
    "    layers.Conv2D(384, kernel_size=(3, 3), strides=(1, 1), padding='same'),\n",
    "    layers.ReLU(),\n",
    "    \n",
    "    layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='same'),\n",
    "    layers.ReLU(),\n",
    "    layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    \n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    layers.Dense(102, activation='softmax')  # 102 classes for Oxford Flowers dataset\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_data, epochs=10, validation_data=validation_data, verbose=1)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(test_data, verbose=1)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
